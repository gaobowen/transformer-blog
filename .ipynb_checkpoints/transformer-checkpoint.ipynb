{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b661ffb8-0c0d-47ef-b16f-08a951d1cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O en-zh.tsv \"https://tatoeba.org/zh-cn/exports/download/48735/%E6%9C%89%E8%8B%B1%E8%AF%AD-%E4%B8%AD%E6%96%87%E6%99%AE%E9%80%9A%E8%AF%9D%E5%AF%B9%E5%BA%94%E5%8F%A5%20-%202024-06-11.tsv\"\n",
    "# 将繁体转成简体\n",
    "# !pip install zhconv\n",
    "# !pip install nltk #英文分词\n",
    "# !pip install jieba #中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca5673b-6fd7-44c7-a339-a4e8f7c7eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zhconv in /home/ubuntu/miniconda3/envs/ernerf/lib/python3.10/site-packages (1.4.3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3211579/2242610548.py:50: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  all_data = all_data.applymap(lambda x: x.lower()) #英文全部转为小写\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>zhs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have to go to sleep.</td>\n",
       "      <td>我该去睡觉了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>today is june 18th and it is muiriel's birthday!</td>\n",
       "      <td>今天是6月18号，也是muiriel的生日！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>muiriel is 20 now.</td>\n",
       "      <td>muiriel现在20岁了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the password is \"muiriel\".</td>\n",
       "      <td>密码是\"muiriel\"。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i will be back soon.</td>\n",
       "      <td>我很快就会回来。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68673</th>\n",
       "      <td>i can say for certain that my mother's cooking...</td>\n",
       "      <td>我能肯定地说我妈的厨艺算不上好。我喜欢调味得当的食物，像我太太的。但我仍然想让我妈教我做饭，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68674</th>\n",
       "      <td>you can read this book.</td>\n",
       "      <td>你可以看这书。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68675</th>\n",
       "      <td>you may read this book.</td>\n",
       "      <td>你可以看这书。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68676</th>\n",
       "      <td>she lives by the sea, but she doesn't know how...</td>\n",
       "      <td>她住海边，可还是不会游泳。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68677</th>\n",
       "      <td>one idiot can ask more questions than ten wise...</td>\n",
       "      <td>一个傻瓜问出的问题，能多到连十个智者都顾不及回答。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68678 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      en  \\\n",
       "0                                 i have to go to sleep.   \n",
       "1       today is june 18th and it is muiriel's birthday!   \n",
       "2                                     muiriel is 20 now.   \n",
       "3                             the password is \"muiriel\".   \n",
       "4                                   i will be back soon.   \n",
       "...                                                  ...   \n",
       "68673  i can say for certain that my mother's cooking...   \n",
       "68674                            you can read this book.   \n",
       "68675                            you may read this book.   \n",
       "68676  she lives by the sea, but she doesn't know how...   \n",
       "68677  one idiot can ask more questions than ten wise...   \n",
       "\n",
       "                                                     zhs  \n",
       "0                                                我该去睡觉了。  \n",
       "1                                 今天是6月18号，也是muiriel的生日！  \n",
       "2                                         muiriel现在20岁了。  \n",
       "3                                          密码是\"muiriel\"。  \n",
       "4                                               我很快就会回来。  \n",
       "...                                                  ...  \n",
       "68673  我能肯定地说我妈的厨艺算不上好。我喜欢调味得当的食物，像我太太的。但我仍然想让我妈教我做饭，...  \n",
       "68674                                            你可以看这书。  \n",
       "68675                                            你可以看这书。  \n",
       "68676                                      她住海边，可还是不会游泳。  \n",
       "68677                          一个傻瓜问出的问题，能多到连十个智者都顾不及回答。  \n",
       "\n",
       "[68678 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "\n",
    "from zhconv import convert\n",
    "\n",
    "\n",
    "def Q2B(uchar):\n",
    "  \"\"\"判断一个unicode是否是全角数字\"\"\"\n",
    "  if uchar >= u'\\uff10' and uchar <= u'\\uff19':\n",
    "    \"\"\"单个字符 全角转半角\"\"\"\n",
    "    inside_code = ord(uchar)\n",
    "    if inside_code == 0x3000:\n",
    "        inside_code = 0x0020\n",
    "    else:\n",
    "        inside_code -= 0xfee0\n",
    "    if inside_code < 0x0020 or inside_code > 0x7e: #转完之后不是半角字符返回原来的字符\n",
    "        return uchar\n",
    "    return chr(inside_code)\n",
    "  else:\n",
    "    return uchar\n",
    "\n",
    "def stringpartQ2B(ustring):\n",
    "  return \"\".join([Q2B(uchar) for uchar in ustring])\n",
    "\n",
    "\n",
    "def convertSimple(x):\n",
    "  return stringpartQ2B(convert(x.values[0], 'zh-cn'))\n",
    "\n",
    "\n",
    "all_data = pd.read_csv('en-zh.tsv',sep='\\t',on_bad_lines='skip',names=['NO.1','en','NO.2','zh'])\n",
    "\n",
    "zh_data = all_data.iloc[:,[3]].apply(convertSimple, axis=1).rename('zhs',inplace=True)\n",
    "\n",
    "all_data = pd.concat([all_data.iloc[:,[1]], zh_data], axis=1)\n",
    "\n",
    "all_data = all_data.applymap(lambda x: x.lower()) #英文全部转为小写\n",
    "\n",
    "print(type(all_data))\n",
    "print(type(all_data.values))\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf4a20a-f154-494a-ad29-aa5278827e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ubuntu/miniconda3/envs/ernerf/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/miniconda3/envs/ernerf/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/miniconda3/envs/ernerf/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/miniconda3/envs/ernerf/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/envs/ernerf/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: jieba in /home/ubuntu/miniconda3/envs/ernerf/lib/python3.10/site-packages (0.42.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.761 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/tmp/ipykernel_3211579/2293653728.py:11: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  en_list = all_data.iloc[:,[0]].applymap(lambda x: nltk.word_tokenize(x))\n",
      "/tmp/ipykernel_3211579/2293653728.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  zhs_list = all_data.iloc[:,[1]].applymap(lambda x: jieba.lcut(x))\n",
      "/tmp/ipykernel_3211579/2293653728.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  all_data = all_data.applymap(lambda x: ['<BOF>']+ x +['<EOF>'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>zhs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[&lt;BOF&gt;, i, have, to, go, to, sleep, ., &lt;EOF&gt;]</td>\n",
       "      <td>[&lt;BOF&gt;, 我, 该, 去, 睡觉, 了, 。, &lt;EOF&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[&lt;BOF&gt;, today, is, june, 18th, and, it, is, mu...</td>\n",
       "      <td>[&lt;BOF&gt;, 今天, 是, 6, 月, 18, 号, ，, 也, 是, muiriel, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[&lt;BOF&gt;, muiriel, is, 20, now, ., &lt;EOF&gt;]</td>\n",
       "      <td>[&lt;BOF&gt;, muiriel, 现在, 20, 岁, 了, 。, &lt;EOF&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[&lt;BOF&gt;, the, password, is, ``, muiriel, '', .,...</td>\n",
       "      <td>[&lt;BOF&gt;, 密码, 是, \", muiriel, \", 。, &lt;EOF&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[&lt;BOF&gt;, i, will, be, back, soon, ., &lt;EOF&gt;]</td>\n",
       "      <td>[&lt;BOF&gt;, 我, 很快, 就, 会, 回来, 。, &lt;EOF&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68673</th>\n",
       "      <td>[&lt;BOF&gt;, i, can, say, for, certain, that, my, m...</td>\n",
       "      <td>[&lt;BOF&gt;, 我能, 肯定, 地, 说, 我, 妈, 的, 厨艺, 算不上, 好, 。, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68674</th>\n",
       "      <td>[&lt;BOF&gt;, you, can, read, this, book, ., &lt;EOF&gt;]</td>\n",
       "      <td>[&lt;BOF&gt;, 你, 可以, 看, 这书, 。, &lt;EOF&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68675</th>\n",
       "      <td>[&lt;BOF&gt;, you, may, read, this, book, ., &lt;EOF&gt;]</td>\n",
       "      <td>[&lt;BOF&gt;, 你, 可以, 看, 这书, 。, &lt;EOF&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68676</th>\n",
       "      <td>[&lt;BOF&gt;, she, lives, by, the, sea, ,, but, she,...</td>\n",
       "      <td>[&lt;BOF&gt;, 她, 住, 海边, ，, 可, 还是, 不会, 游泳, 。, &lt;EOF&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68677</th>\n",
       "      <td>[&lt;BOF&gt;, one, idiot, can, ask, more, questions,...</td>\n",
       "      <td>[&lt;BOF&gt;, 一个, 傻瓜, 问出, 的, 问题, ，, 能, 多, 到, 连, 十个, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68678 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      en  \\\n",
       "0          [<BOF>, i, have, to, go, to, sleep, ., <EOF>]   \n",
       "1      [<BOF>, today, is, june, 18th, and, it, is, mu...   \n",
       "2                [<BOF>, muiriel, is, 20, now, ., <EOF>]   \n",
       "3      [<BOF>, the, password, is, ``, muiriel, '', .,...   \n",
       "4             [<BOF>, i, will, be, back, soon, ., <EOF>]   \n",
       "...                                                  ...   \n",
       "68673  [<BOF>, i, can, say, for, certain, that, my, m...   \n",
       "68674      [<BOF>, you, can, read, this, book, ., <EOF>]   \n",
       "68675      [<BOF>, you, may, read, this, book, ., <EOF>]   \n",
       "68676  [<BOF>, she, lives, by, the, sea, ,, but, she,...   \n",
       "68677  [<BOF>, one, idiot, can, ask, more, questions,...   \n",
       "\n",
       "                                                     zhs  \n",
       "0                      [<BOF>, 我, 该, 去, 睡觉, 了, 。, <EOF>]  \n",
       "1      [<BOF>, 今天, 是, 6, 月, 18, 号, ，, 也, 是, muiriel, ...  \n",
       "2               [<BOF>, muiriel, 现在, 20, 岁, 了, 。, <EOF>]  \n",
       "3                [<BOF>, 密码, 是, \", muiriel, \", 。, <EOF>]  \n",
       "4                     [<BOF>, 我, 很快, 就, 会, 回来, 。, <EOF>]  \n",
       "...                                                  ...  \n",
       "68673  [<BOF>, 我能, 肯定, 地, 说, 我, 妈, 的, 厨艺, 算不上, 好, 。, ...  \n",
       "68674                    [<BOF>, 你, 可以, 看, 这书, 。, <EOF>]  \n",
       "68675                    [<BOF>, 你, 可以, 看, 这书, 。, <EOF>]  \n",
       "68676      [<BOF>, 她, 住, 海边, ，, 可, 还是, 不会, 游泳, 。, <EOF>]  \n",
       "68677  [<BOF>, 一个, 傻瓜, 问出, 的, 问题, ，, 能, 多, 到, 连, 十个, ...  \n",
       "\n",
       "[68678 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.word_tokenize(\"today is june 18th and it is muiriel's birthday!\")\n",
    "\n",
    "import jieba\n",
    "jieba.lcut(\"今天是6月18号，也是muiriel的生日！\")\n",
    "\n",
    "en_list = all_data.iloc[:,[0]].applymap(lambda x: nltk.word_tokenize(x))\n",
    "zhs_list = all_data.iloc[:,[1]].applymap(lambda x: jieba.lcut(x))\n",
    "all_data = pd.concat([en_list,zhs_list],axis=1)\n",
    "\n",
    "# 添加两个特殊符号 <BOF> <EOF> 分别表示 句子的开始和结束\n",
    "all_data = all_data.applymap(lambda x: ['<BOF>']+ x +['<EOF>'])\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f90cc-3180-4509-a3c1-22dd963211af",
   "metadata": {},
   "source": [
    "统计单词的频率，降序排列，用单词的下标作为单词的id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6c7541-263f-417e-b324-6483a64c8ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<PAD>', 0), ('<UNK>', 1), ('<BOF>', 2), ('<EOF>', 3), ('.', 4), ('the', 5), ('i', 6), ('to', 7), ('you', 8), ('is', 9), ('a', 10), ('?', 11), (',', 12), ('he', 13), (\"n't\", 14), ('in', 15), ('do', 16), ('of', 17), ('it', 18), (\"'s\", 19)]\n",
      "[(0, '<PAD>'), (1, '<UNK>'), (2, '<BOF>'), (3, '<EOF>'), (4, '.'), (5, 'the'), (6, 'i'), (7, 'to'), (8, 'you'), (9, 'is'), (10, 'a'), (11, '?'), (12, ','), (13, 'he'), (14, \"n't\"), (15, 'in'), (16, 'do'), (17, 'of'), (18, 'it'), (19, \"'s\")]\n",
      "=========\n",
      "[('<PAD>', 0), ('<UNK>', 1), ('<BOF>', 2), ('<EOF>', 3), ('。', 4), ('我', 5), ('的', 6), ('了', 7), ('你', 8), ('他', 9), ('，', 10), ('？', 11), ('是', 12), ('在', 13), ('她', 14), ('汤姆', 15), ('吗', 16), ('我们', 17), ('不', 18), ('很', 19)]\n",
      "[(0, '<PAD>'), (1, '<UNK>'), (2, '<BOF>'), (3, '<EOF>'), (4, '。'), (5, '我'), (6, '的'), (7, '了'), (8, '你'), (9, '他'), (10, '，'), (11, '？'), (12, '是'), (13, '在'), (14, '她'), (15, '汤姆'), (16, '吗'), (17, '我们'), (18, '不'), (19, '很')]\n",
      "=========\n",
      "英文词典长度 14738\n",
      "中文词典长度 25706\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#统计单词的频率\n",
    "en_table = Counter([])\n",
    "zhs_table = Counter([])\n",
    "\n",
    "for row in all_data.values:\n",
    "  en_table.update(row[0])\n",
    "  zhs_table.update(row[1])\n",
    "\n",
    "en_to_id = {\"<PAD>\": 0, \"<UNK>\": 1,} # UNK表示未知字符，PAD表示占位符\n",
    "zhs_to_id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "\n",
    "id_to_en = {0: '<PAD>', 1:'<UNK>'}\n",
    "id_to_zhs = {0: '<PAD>', 1:'<UNK>'}\n",
    "\n",
    "#用数组下标作为单词的id，因为 0: '<PAD>', 1:'<UNK>' 所以id从2开始\n",
    "for idx,ele in enumerate(en_table.most_common(50000), start=2):\n",
    "  en_to_id[ele[0]] = idx\n",
    "  id_to_en[idx] = ele[0]\n",
    "\n",
    "print(list(en_to_id.items())[:20])\n",
    "print(list(id_to_en.items())[:20])\n",
    "print(\"=========\")\n",
    "\n",
    "for idx,ele in enumerate(zhs_table.most_common(50000), start=2):\n",
    "  zhs_to_id[ele[0]] = idx\n",
    "  id_to_zhs[idx] = ele[0]\n",
    "\n",
    "print(list(zhs_to_id.items())[:20])\n",
    "print(list(id_to_zhs.items())[:20])\n",
    "\n",
    "print(\"=========\")\n",
    "print('英文词典长度', len(list(en_to_id.items())))\n",
    "print('中文词典长度', len(list(zhs_to_id.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba9447a-5e21-404b-b204-0693dc615314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3211579/2257538610.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  all_en_id = all_data.iloc[:,[0]].applymap(lambda x: [en_to_id.get(word, en_to_id['<UNK>']) for word in x])\n",
      "/tmp/ipykernel_3211579/2257538610.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  all_zhs_id = all_data.iloc[:,[1]].applymap(lambda x: [zhs_to_id.get(word, zhs_to_id['<UNK>']) for word in x])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>zhs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2, 6, 24, 7, 50, 7, 337, 4, 3]</td>\n",
       "      <td>[2, 5, 160, 22, 244, 7, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 119, 9, 1703, 5554, 31, 18, 9, 2983, 19, 5...</td>\n",
       "      <td>[2, 76, 12, 1487, 377, 2541, 1027, 10, 48, 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, 2983, 9, 1287, 95, 4, 3]</td>\n",
       "      <td>[2, 3548, 66, 1537, 580, 7, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2, 5, 2172, 9, 135, 2983, 137, 4, 3]</td>\n",
       "      <td>[2, 2176, 12, 307, 3548, 307, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2, 6, 48, 38, 140, 211, 4, 3]</td>\n",
       "      <td>[2, 5, 253, 39, 28, 166, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68673</th>\n",
       "      <td>[2, 6, 42, 147, 29, 1150, 21, 23, 167, 19, 932...</td>\n",
       "      <td>[2, 276, 445, 123, 25, 5, 1591, 6, 2871, 8913,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68674</th>\n",
       "      <td>[2, 8, 42, 202, 22, 111, 4, 3]</td>\n",
       "      <td>[2, 8, 51, 73, 9208, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68675</th>\n",
       "      <td>[2, 8, 160, 202, 22, 111, 4, 3]</td>\n",
       "      <td>[2, 8, 51, 73, 9208, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68676</th>\n",
       "      <td>[2, 26, 363, 64, 5, 799, 12, 75, 26, 66, 14, 5...</td>\n",
       "      <td>[2, 14, 118, 1508, 10, 565, 132, 70, 225, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68677</th>\n",
       "      <td>[2, 70, 949, 42, 304, 94, 617, 89, 243, 1619, ...</td>\n",
       "      <td>[2, 26, 1367, 25703, 6, 94, 10, 60, 104, 53, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68678 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      en  \\\n",
       "0                        [2, 6, 24, 7, 50, 7, 337, 4, 3]   \n",
       "1      [2, 119, 9, 1703, 5554, 31, 18, 9, 2983, 19, 5...   \n",
       "2                           [2, 2983, 9, 1287, 95, 4, 3]   \n",
       "3                  [2, 5, 2172, 9, 135, 2983, 137, 4, 3]   \n",
       "4                         [2, 6, 48, 38, 140, 211, 4, 3]   \n",
       "...                                                  ...   \n",
       "68673  [2, 6, 42, 147, 29, 1150, 21, 23, 167, 19, 932...   \n",
       "68674                     [2, 8, 42, 202, 22, 111, 4, 3]   \n",
       "68675                    [2, 8, 160, 202, 22, 111, 4, 3]   \n",
       "68676  [2, 26, 363, 64, 5, 799, 12, 75, 26, 66, 14, 5...   \n",
       "68677  [2, 70, 949, 42, 304, 94, 617, 89, 243, 1619, ...   \n",
       "\n",
       "                                                     zhs  \n",
       "0                          [2, 5, 160, 22, 244, 7, 4, 3]  \n",
       "1      [2, 76, 12, 1487, 377, 2541, 1027, 10, 48, 12,...  \n",
       "2                      [2, 3548, 66, 1537, 580, 7, 4, 3]  \n",
       "3                    [2, 2176, 12, 307, 3548, 307, 4, 3]  \n",
       "4                         [2, 5, 253, 39, 28, 166, 4, 3]  \n",
       "...                                                  ...  \n",
       "68673  [2, 276, 445, 123, 25, 5, 1591, 6, 2871, 8913,...  \n",
       "68674                         [2, 8, 51, 73, 9208, 4, 3]  \n",
       "68675                         [2, 8, 51, 73, 9208, 4, 3]  \n",
       "68676    [2, 14, 118, 1508, 10, 565, 132, 70, 225, 4, 3]  \n",
       "68677  [2, 26, 1367, 25703, 6, 94, 10, 60, 104, 53, 6...  \n",
       "\n",
       "[68678 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把翻译数据全部转成id的表示形式\n",
    "all_en_id = all_data.iloc[:,[0]].applymap(lambda x: [en_to_id.get(word, en_to_id['<UNK>']) for word in x]) \n",
    "all_zhs_id = all_data.iloc[:,[1]].applymap(lambda x: [zhs_to_id.get(word, zhs_to_id['<UNK>']) for word in x]) \n",
    "all_data = pd.concat([all_en_id, all_zhs_id], axis=1)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916df557-8e7b-422e-bbfa-ec0c2c5676bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据每个句子的长度，进行padding，使每行的长度一致\n",
    "# torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)\n",
    "# sequences(list[Tensor]) 每行的数据长短不一的list\n",
    "# batch_first batch_size是否在第一维度\n",
    "# padding_value 填充值\n",
    "# a = torch.Tensor([1])\n",
    "# b = torch.Tensor([2,2])\n",
    "# c = torch.Tensor([3,3,3])\n",
    "# d = torch.Tensor([4,4,4,4,4,4])\n",
    "# pad_test = torch.nn.utils.rnn.pad_sequence([a,b,c,d],batch_first=True)\n",
    "# pad_test\n",
    "#torch.Tensor([[1, 1],[2,2]])\n",
    "\n",
    "# tensor([[1., 0., 0., 0., 0., 0.],\n",
    "#         [2., 2., 0., 0., 0., 0.],\n",
    "#         [3., 3., 3., 0., 0., 0.],\n",
    "#         [4., 4., 4., 4., 4., 4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bf502934-0b4d-48a2-acbd-a61d51f5e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个数据加载器，每次获取固定数量的batch，同时对batch进行补齐\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, en_to_id, id_to_en, zhs_to_id, id_to_zhs, device=\"cuda\"):\n",
    "        self.data = data\n",
    "        self.en_to_id = en_to_id\n",
    "        self.id_to_en = id_to_en\n",
    "        self.zhs_to_id = zhs_to_id\n",
    "        self.id_to_zhs = id_to_zhs\n",
    "        self.PAD = en_to_id['<PAD>']\n",
    "        self.device = device\n",
    "\n",
    "        self.en_data = [torch.Tensor(list(map(float,en[0]))) for en in self.data.iloc[:,[0]].values]\n",
    "        self.zh_data = [torch.Tensor(list(map(float,zh[0]))) for zh in self.data.iloc[:,[1]].values]\n",
    "\n",
    "        assert len(self.en_data) == len(self.zh_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_ids = self.en_data[idx]\n",
    "        zh_ids = self.zh_data[idx]\n",
    "        return [en_ids, zh_ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pad_mask(seq, pad_idx):\n",
    "        # 对 PAD 做屏蔽操作\n",
    "        # batch * seqlen -> batch * 1 * seqlen\n",
    "        return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_subsequent_mask(seq):\n",
    "        # decode 不能让后面单词的注意力影响到前面的单词\n",
    "        batch_size, seq_len = seq.size()\n",
    "        # torch.triu 保留上三角元素，diagonal=0保留对角线，diagonal=1不保留对角线\n",
    "        # 保留下三角，包括对角线。因为 decoder输入的是完整句子，对 Q 做屏蔽，使得K看不到Q后面的单词\n",
    "        # Attention = Q @ K^T = Q (seq_len_q x d) @ K^T (d x seq_len_k) = (seq_len_q x seq_len_k) \n",
    "        '''\n",
    "            K(memory)\n",
    "         -----------\n",
    "         | [1, 0, 0]\n",
    "        Q| [1, 1, 0]\n",
    "         | [1, 1, 1]\n",
    "        '''\n",
    "        subsequent_mask = (1 - torch.triu(\n",
    "            torch.ones((1, seq_len, seq_len), device=seq.device), diagonal=1)).bool()\n",
    "        return subsequent_mask\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        en_ids = [torch.LongTensor(np.array(x[0])) for x in batch]\n",
    "        zh_ids = [torch.LongTensor(np.array(x[1])) for x in batch]\n",
    "        \n",
    "        batch_en = torch.nn.utils.rnn.pad_sequence(en_ids, batch_first=True, padding_value=self.PAD)\n",
    "        batch_zh = torch.nn.utils.rnn.pad_sequence(zh_ids, batch_first=True, padding_value=self.PAD)\n",
    "        batch_en = batch_en.to(self.device)\n",
    "        batch_zh = batch_zh.to(self.device)\n",
    "        \n",
    "        # 训练过程中采用 Teacher Forcing 模式, 将目标句子拆成 “<BOF>我吃午饭” 和 “我吃午饭<EOF>” 直接用目标输入去推理目标输出\n",
    "        tgt_in = batch_zh[:,:-1]\n",
    "        tgt_real = batch_zh[:,1:]\n",
    "        src = batch_en\n",
    "\n",
    "        # torch中的attention mask 尺寸 correct_3d_size = (bsz * num_heads, tgt_len, src_len)  => n k q\n",
    "        # src 中要排除 PAD 对注意力的干扰\n",
    "        src_mask = self.get_pad_mask(src, self.PAD)\n",
    "        \n",
    "\n",
    "        # decode 中要排除 PAD 和 后面单词 对注意力的干扰\n",
    "        tgt_mask = self.get_pad_mask(tgt_in, self.PAD) & self.get_subsequent_mask(tgt_in)\n",
    "\n",
    "        # torch.nn.MultiheadAttention 中的的mask True代表屏蔽，False表示通过\n",
    "        # n_head = 4\n",
    "        # src_mask = src_mask.repeat(n_head, 1, 1)\n",
    "        # tgt_mask = tgt_mask.repeat(n_head, 1, 1)\n",
    "        # src_mask = src_mask == False\n",
    "        # tgt_mask = tgt_mask == False\n",
    "        \n",
    "        # print('src_mask', src_mask.shape, 'tgt_mask', tgt_mask.shape)\n",
    "        return src, src_mask, tgt_in, tgt_real, tgt_mask\n",
    "\n",
    "dataset = MyDataset(all_data, en_to_id, id_to_en, zhs_to_id, id_to_zhs)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=2,\n",
    "                                  collate_fn=dataset.collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    # print(batch)\n",
    "    break\n",
    "\n",
    "\n",
    "# raise OSError('1111')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9945da51-70e1-48cc-b6fe-1fa6b4aa63c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super(InputEmbedding, self).__init__()\n",
    "    # 一个普通的 embedding 层, vocab_size词表长度， d_model每个单词的维度 \n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.d_model = d_model\n",
    "\n",
    "  def forward(self,x):\n",
    "    # 这里x的尺寸为 batch_size(句子个数) * seq_len（每句单词个数） * d_model（单词维度）\n",
    "    x = self.embedding(x) * math.sqrt(self.d_model) \n",
    "    # print('self.embedding(x)', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_len=500, device='cuda'):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    # 初始化max_len×d_model的全零矩阵\n",
    "    # print('max_len', max_len, 'd_model', d_model)\n",
    "    pe = torch.zeros(max_len, d_model, device=device)\n",
    "\n",
    "    \"\"\"\n",
    "    unsqueeze作用：扩展维度\n",
    "    x = torch.Tensor([1, 2, 3, 4]) #torch.Size([4]) 一维\n",
    "    torch.unsqueeze(x, 0)) #tensor([[1., 2., 3., 4.]]) torch.Size([1, 4]) 一行四列，二维\n",
    "    torch.unsqueeze(x, 1))\n",
    "    tensor([[1.],\n",
    "        [2.],\n",
    "        [3.],\n",
    "        [4.]]) #torch.Size([4, 1]) 四行一列\n",
    "    \"\"\"\n",
    "    # 上面公式 P(i,2j) 与 P(i,2j+1) 中，i∈[0, max_len)表示单词在句子中的位置 2j∈[0, d_model)表示单词的维度\n",
    "    position = torch.arange(0., max_len, device=device).unsqueeze(1) # torch.Size([max_len, 1])\n",
    "    # 两个公式中的 i/(10000^(2j/d_model)) 项是相同的，只需要计算一次即可\n",
    "    # 这里幂运算太多，我们使用exp和log来转换实现公式中 i要除以的分母（由于是分母，要注意带负号）\n",
    "    # torch.exp自然数e为底指数运算 与 math.log对数运算抵消\n",
    "    div_term = torch.exp(torch.arange(0., d_model, 2, device=device) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "    # 根据公式，计算各个位置在各embedding维度上的位置纹理值，存放到pe矩阵中\n",
    "    pe[:, 0::2] = torch.sin(position * div_term) #P(i,2j)=sin(...)，从0开始步长为2 表示2j\n",
    "    pe[:, 1::2] = torch.cos(position * div_term) #P(i,2j+1)=cos(...)，从1开始步长为2 表示2j+1\n",
    "\n",
    "    # 加1个batch维度，使得pe维度变为：1×max_len×d_model，方便后续与一个batch的句子所有词的embedding批量相加\n",
    "    pe = pe.unsqueeze(0)\n",
    "    # 将pe矩阵以持久的buffer状态存下(不会作为要训练的参数)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # 将一个batch的句子所有词的embedding与已构建好的positional embeding相加后输出，且尺寸不变\n",
    "    # 设 x 的句子长度为len，尺寸为 batch×len×d_model, 这里x.size(1)即为句子长度len，则 self.pe[:, :x.size(1)]尺寸为 1×len×d_model\n",
    "    x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # shape = batch * head * len * d_k \n",
    "        # q k 计算点积，所以向量的维度 d_q 和 d_k 需要保证相等\n",
    "        assert q.size(-1) == k.size(-1)\n",
    "\n",
    "        d_k = k.size(-1)\n",
    "\n",
    "        #计算注意力, 这里要计算单词间的关联度，最后两个维度必须保证是 len * d_k\n",
    "        # attn = [batch * head * len * d_k] dot [batch * head * d_k * len]\n",
    "        #      = batch * head * len = batch * head * attn_score\n",
    "        attn = torch.matmul(q / math.sqrt(d_k), k.transpose(-2, -1))\n",
    "\n",
    "        # 如果存在要进行mask的内容，则将注意力评分中需要屏蔽的部分替换成一个很大的负数\n",
    "        if mask is not None:\n",
    "            print(attn.shape, mask.shape)\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        # 对 attn_score 做归一化\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "\n",
    "        \n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        \n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        # k 和 v 通过注意力权重进行交互，所以d_k和d_v可以有不同的维度\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v \n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        # 对 注意力输出 进行特征整理\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 最后的维度size 进行标准化，趋近于标准高斯分布 \n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        mask = attn_mask\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        # batch * len * d_model\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        # 输入尺寸 batch x len x d_model\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        # Transpose for attention dot product: batch x head x len x d_k\n",
    "        # 计算注意力要保证最后两个维度是 句子长度 * 单词维度\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            # batch * len -> batch * 1 * len\n",
    "            # 添加一个 head 维度 1，通过广播机制可以mask所有的head\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        # 把维度 batch x head x len x d_k 还原成 batch x len x d_model, 方便做残差\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "        \n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    # d_model输入特征维度，d_hid 为 PositionwiseFeed 的隐藏层维度\n",
    "    def __init__(self, d_model, d_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_hid, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        # seq_len = src_mask.shape[-1]\n",
    "        # torch中的attention mask 尺寸 correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "        y, attn = self.self_attn(\n",
    "            x, x, x, attn_mask=src_mask)\n",
    "        y = self.ffn(y)\n",
    "        return y, attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.cross_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, encoder_y, tgt_mask=None, cross_attn_mask=None):\n",
    "        # print(x.shape, tgt_mask.shape)\n",
    "\n",
    "        # decoder 自注意力\n",
    "        # tgt_mask2 = torch.zeros((tgt_mask.shape[0], tgt_mask.shape[1], tgt_mask.shape[2]), device=tgt_mask.device).bool()\n",
    "        \n",
    "        decoder_y, decoder_attn = self.self_attn(x, x, x, attn_mask=tgt_mask)\n",
    "        \n",
    "        \n",
    "        knowledge = encoder_y\n",
    "        # 交叉注意力层\n",
    "        # 这里的 decoder_y, encoder_y, encoder_y 理解成 Xq Xk Xv\n",
    "        # 用 decoder 的 q 去 查询 encode 的 k-v 里的关联信息\n",
    "        decoder_y, cross_attn = self.cross_attn(\n",
    "            decoder_y, knowledge, knowledge, attn_mask=cross_attn_mask)\n",
    "        \n",
    "        decoder_y = self.ffn(decoder_y)\n",
    "        return decoder_y, decoder_attn, cross_attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_head, hidden_scaler=4):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % n_head == 0\n",
    "        # 512 / 8 = 64\n",
    "        d_k = d_v = d_model //  n_head\n",
    "\n",
    "        # 输入前 先标准化\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_model * hidden_scaler, n_head, d_k, d_v)\n",
    "            for _ in range(n_layers)])\n",
    "        \n",
    "\n",
    "    def forward(self, src_vecs, src_mask):\n",
    "\n",
    "        encoder_y = self.layer_norm(self.dropout(src_vecs))\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            encoder_y, enc_slf_attn = enc_layer(encoder_y, src_mask)\n",
    "\n",
    "        return encoder_y\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_head, hidden_scaler=4):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % n_head == 0\n",
    "        # 512 / 8 = 64\n",
    "        d_k = d_v = d_model //  n_head\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_model * hidden_scaler, n_head, d_k, d_v)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, tgt_vecs, encoder_y, src_mask, tgt_mask):\n",
    "\n",
    "        dec_output = self.layer_norm(self.dropout(tgt_vecs))\n",
    "        \n",
    "        # print(dec_output)\n",
    "        # 交叉注意力不需要因果掩码，因为其目的是让解码器能够看到编码器的整个输出。但是，它通常需要源序列的填充掩码(src_mask)来忽略无关的填充令牌。\n",
    "        # correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "        # cross_mask = torch.ones((tgt_mask.shape[0], tgt_mask.shape[-1], src_mask.shape[-1]), device=src_mask.device).bool() & src_mask\n",
    "        cross_mask = src_mask.repeat(1, tgt_mask.shape[-1], 1)\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, decoder_attn, cross_attn = dec_layer(\n",
    "                dec_output, encoder_y, tgt_mask=tgt_mask, cross_attn_mask=cross_mask)\n",
    "            \n",
    "        # print(dec_output.shape)\n",
    "        return dec_output\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # vocab: tgt_vocab\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        # decode后的结果，先进入一个全连接层变为词典大小的向量\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.proj(x), dim=-1)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, n_layers, d_model, n_head, DEVICE=\"cuda\"):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.src_embed_pos = nn.Sequential(InputEmbedding(src_vocab, d_model), PositionalEncoding(d_model, device=DEVICE)).to(DEVICE)\n",
    "        self.tgt_embed_pos = nn.Sequential(InputEmbedding(tgt_vocab, d_model), PositionalEncoding(d_model, device=DEVICE)).to(DEVICE)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_head).to(DEVICE)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_head).to(DEVICE)\n",
    "\n",
    "        self.generator = Generator(d_model, tgt_vocab).to(DEVICE)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p) #初始化权重\n",
    "\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed_pos(src), src_mask)\n",
    "\n",
    "    def decode(self, encoder_y, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed_pos(tgt), encoder_y, src_mask, tgt_mask)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # encoder的结果作为decoder的 k-v 参数传入，进行decode\n",
    "        encoder_y = self.encode(src, src_mask)\n",
    "        dec_y = self.decode(encoder_y, src_mask, tgt, tgt_mask)\n",
    "        \n",
    "        return self.generator(dec_y)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8eeee650-5de2-4d96-8d68-5ae6a623b15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin train\n",
      "torch.Size([32, 4, 17, 17]) torch.Size([32, 1, 17, 17])\n",
      "torch.Size([32, 4, 17, 17]) torch.Size([32, 1, 17, 17])\n",
      "torch.Size([32, 4, 17, 17]) torch.Size([32, 1, 17, 17])\n",
      "torch.Size([32, 4, 17, 17]) torch.Size([32, 1, 17, 17])\n",
      "torch.Size([32, 4, 14, 14]) torch.Size([32, 1, 14, 14])\n",
      "torch.Size([32, 4, 14, 17]) torch.Size([32, 1, 238, 17])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (238) must match the size of tensor b (14) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbegin train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m---> 76\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m warmup:\n\u001b[1;32m     78\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep(epoch_loss)\n",
      "Cell \u001b[0;32mIn[111], line 51\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     50\u001b[0m     src, src_mask, tgt_in, tgt_real, tgt_mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 51\u001b[0m     tgt_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# 展平为 [batch_size * seq_length, vocab_size]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     output_reshape \u001b[38;5;241m=\u001b[39m tgt_out\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tgt_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/ernerf/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[110], line 307\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt, src_mask, tgt_mask):\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# encoder的结果作为decoder的 k-v 参数传入，进行decode\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     encoder_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(src, src_mask)\n\u001b[0;32m--> 307\u001b[0m     dec_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(dec_y)\n",
      "Cell \u001b[0;32mIn[110], line 302\u001b[0m, in \u001b[0;36mTransformer.decode\u001b[0;34m(self, encoder_y, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder_y, src_mask, tgt, tgt_mask):\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgt_embed_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ernerf/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[110], line 263\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, tgt_vecs, encoder_y, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    261\u001b[0m cross_mask \u001b[38;5;241m=\u001b[39m src_mask\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, tgt_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_stack:\n\u001b[0;32m--> 263\u001b[0m     dec_output, decoder_attn, cross_attn \u001b[38;5;241m=\u001b[39m \u001b[43mdec_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdec_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_attn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# print(dec_output.shape)\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dec_output\n",
      "File \u001b[0;32m~/miniconda3/envs/ernerf/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[110], line 204\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, encoder_y, tgt_mask, cross_attn_mask)\u001b[0m\n\u001b[1;32m    200\u001b[0m knowledge \u001b[38;5;241m=\u001b[39m encoder_y\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# 交叉注意力层\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# 这里的 decoder_y, encoder_y, encoder_y 理解成 Xq Xk Xv\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# 用 decoder 的 q 去 查询 encode 的 k-v 里的关联信息\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m decoder_y, cross_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknowledge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknowledge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m decoder_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(decoder_y)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoder_y, decoder_attn, cross_attn\n",
      "File \u001b[0;32m~/miniconda3/envs/ernerf/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[110], line 139\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, attn_mask)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# batch * len -> batch * 1 * len\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# 添加一个 head 维度 1，通过广播机制可以mask所有的head\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# For head axis broadcasting.\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m q, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Transpose to move the head dimension back: b x lq x n x dv\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# 把维度 batch x head x len x d_k 还原成 batch x len x d_model, 方便做残差\u001b[39;00m\n\u001b[1;32m    144\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(sz_b, len_q, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ernerf/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[110], line 83\u001b[0m, in \u001b[0;36mScaledDotProductAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(attn\u001b[38;5;241m.\u001b[39mshape, mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 83\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1e9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# 对 attn_score 做归一化\u001b[39;00m\n\u001b[1;32m     85\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(F\u001b[38;5;241m.\u001b[39msoftmax(attn, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (238) must match the size of tensor b (14) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "train_dataset = MyDataset(all_data, en_to_id, id_to_en, zhs_to_id, id_to_zhs, device=DEVICE)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=32,\n",
    "                                  collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "test_dataset = MyDataset(all_data, en_to_id, id_to_en, zhs_to_id, id_to_zhs, device=DEVICE)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=32,\n",
    "                                  collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "src_vocab = len(en_to_id)\n",
    "tgt_vocab = len(zhs_to_id)\n",
    "\n",
    "# 这里数据样本比较少,只用4层 256 4\n",
    "model = Transformer(src_vocab, tgt_vocab, n_layers=4, d_model=256, n_head=4, DEVICE=DEVICE)\n",
    "\n",
    "# optimizer parameter setting\n",
    "init_lr = 1e-6\n",
    "warmup = 8\n",
    "epoch = 100\n",
    "clip = 1.0\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=init_lr)\n",
    "\n",
    "# 用于动态控制学习率的大小\n",
    "# 在发现loss不再降低或者acc不再提高之后，降低学习率。\n",
    "# factor触发条件后lr*=factor；\n",
    "# patience不再减小（或增大）的累计次数；\n",
    "patience = 7\n",
    "factor = 0.9\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.PAD, reduction='sum')  \n",
    "\n",
    "def train_epoch(model, data, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in data:\n",
    "        \n",
    "        src, src_mask, tgt_in, tgt_real, tgt_mask = batch\n",
    "        tgt_out = model(src, tgt_in, src_mask, tgt_mask)\n",
    "        # 展平为 [batch_size * seq_length, vocab_size]\n",
    "        output_reshape = tgt_out.contiguous().view(-1, tgt_out.shape[-1])\n",
    "        \n",
    "        # 展平为 [batch_size * seq_length]\n",
    "        tgt_real = tgt_real.contiguous().view(-1)\n",
    "        ntokens = (tgt_real != train_dataset.PAD).data.sum() #去掉 PAD 计算token平均损失\n",
    "        # 损失函数可以逐个元素地计算交叉熵，可以类比成 “广播”\n",
    "        loss = criterion(output_reshape, tgt_real) / ntokens\n",
    "        loss.backward() # 反向\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step() #更新权重\n",
    "        optimizer.zero_grad() #梯度清零\n",
    "        \n",
    "        epoch_loss += loss.item() #直接获得对应的python数据类型\n",
    "        if(math.isnan(epoch_loss)):\n",
    "            raise OSError(epoch_loss)\n",
    "            \n",
    "\n",
    "        \n",
    "    return epoch_loss / len(data)\n",
    "\n",
    "print('begin train')\n",
    "\n",
    "for step in range(epoch):\n",
    "    epoch_loss = train_epoch(model, train_dataloader, optimizer, criterion, clip)\n",
    "    if step > warmup:\n",
    "        scheduler.step(epoch_loss)\n",
    "    print('epoch_loss', epoch_loss)\n",
    "\n",
    "print('end train')\n",
    "\n",
    "torch.save(model.state_dict(), 'model-1000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "608a7708-6ab2-4c8e-b3b1-b9a7470697af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: thulac in /home/ubuntu/miniconda3/envs/ernerf/lib/python3.10/site-packages (0.2.2)\n",
      "Model loaded succeed\n",
      "[['今天', ''], ['是', ''], ['6月', ''], ['18号', ''], ['，', ''], ['也', ''], ['是', ''], ['muiriel', ''], ['的', ''], ['生日', ''], ['！', '']]\n",
      "['today', 'is', 'june', '18th', 'and', 'it', 'is', 'muiriel', \"'s\", 'birthday', '!']\n",
      "['今天', '是', '6', '月', '18', '号', '，', '也', '是', 'muiriel', '的', '生日', '！']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install thulac\n",
    "\n",
    "# import thulac   \n",
    "\n",
    "# thu = thulac.thulac(seg_only=True)  # 只进行分词，不进行词性标注\n",
    "# sentence = \"今天是6月18号，也是muiriel的生日！\"\n",
    "# tokens = thu.cut(sentence)\n",
    "# print(tokens)\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# print(nltk.word_tokenize(\"today is june 18th and it is muiriel's birthday!\"))\n",
    "\n",
    "# import jieba\n",
    "# print(jieba.lcut(\"今天是6月18号，也是muiriel的生日！\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2f029-0509-4570-839e-4769dfe07176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
